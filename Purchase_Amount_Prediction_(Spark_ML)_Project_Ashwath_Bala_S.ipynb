{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Purchase Amount Prediction (Spark ML) Project"
      ],
      "metadata": {
        "id": "yChCz-_3daZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Done By: Ashwath Bala S"
      ],
      "metadata": {
        "id": "EXmuRgj5Z4vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table of Contents:\n",
        "\n",
        "1.Problem Statement\n",
        "\n",
        "2.Data Dictionary\n",
        "\n",
        "3.Loading the Required Libraries\n",
        "\n",
        "4.Loading the Train and Test Dataset\n",
        "\n",
        "5.Univariate Analysis of the Target Variable\n",
        "\n",
        "6.Missing Value Treatment\n",
        "\n",
        "7.Label Encoding\n",
        "\n",
        "8.One-Hot Encoding\n",
        "\n",
        "9.Data Transformation\n",
        "\n",
        "10.Model Building\n",
        "\n",
        "11.Making Predictions on the Test Data\n",
        "\n",
        "12.Grid Search CV - Model Improvement\n",
        "\n",
        "13.Making Predictions on the Best Model\n",
        "\n",
        "14.Spark ML Pipeline\n"
      ],
      "metadata": {
        "id": "RwFbybaiZ7Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Problem Statement\n",
        "\n",
        "![Purchase Amount Prediction.png](data:image/png;base64,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)\n",
        "\n",
        " A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.\n",
        "\n",
        "The data set also contains customer demographics (age, gender, marital status,city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
        "\n",
        "Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products."
      ],
      "metadata": {
        "id": "D4Ujt8Nrc-m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Data Dictionary\n",
        "\n",
        "User_ID: A unique identifier for each user.\n",
        "\n",
        "Product_ID: A unique identifier for each product.\n",
        "\n",
        "Gender: The gender of the customer.\n",
        "\n",
        "Age: The age range of the user.\n",
        "\n",
        "Occupation: The occupation code of the user.\n",
        "\n",
        "City_Category: The category of the city.\n",
        "\n",
        "Stay_In_Current_City_Years: The number of years the user has stayed in the current city.\n",
        "\n",
        "Marital_Status: The marital status of the user.\n",
        "\n",
        "Product_Category_1: The main product category code.\n",
        "\n",
        "Product_Category_2: The secondary product category code.\n",
        "\n",
        "Product_Category_3: The tertiary product category code.\n",
        "\n",
        "Purchase: The purchase amount (target variable)."
      ],
      "metadata": {
        "id": "WXfTiVvFeEsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Loading the Required Libraries"
      ],
      "metadata": {
        "id": "sR7-2sjLaHW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY9a_SwvbJ28",
        "outputId": "ac808c0d-3363-46b4-990c-4ab687c08049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=89ffa837292165b3ba877a5a98f56635c0029c9525f04b8aa33ba8ed421a39f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Installing PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Necessary Libraries\n",
        "\n",
        "from pyspark import keyword_only\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline,Transformer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "from pyspark.sql.functions import when, col, log1p"
      ],
      "metadata": {
        "id": "RebXD_Dkc5JY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Loading the Train and Test Dataset"
      ],
      "metadata": {
        "id": "O_6VBx1aeOKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark session\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "ZKhmlP84dCT-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the data\n",
        "\n",
        "# Reading the train data\n",
        "train_data = spark.read.csv(\"train.csv\",inferSchema=True, header=True)\n",
        "\n",
        "# Reading the test data\n",
        "test_data  = spark.read.csv(\"test.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "17bVCDF9dIZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the Schema\n",
        "\n",
        "# Data type of the train data columns\n",
        "train_data.printSchema()"
      ],
      "metadata": {
        "id": "e-1F3NaeeWU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798616bc-f334-49ed-93a8-86c6c21888e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type of the test data columns\n",
        "test_data.printSchema()"
      ],
      "metadata": {
        "id": "nQT3EV0pehLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e17591-c7f3-40c3-c592-3f26593a4130"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Univariate Analysis of Target Variable"
      ],
      "metadata": {
        "id": "V6gdsvk4eV2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Analysis for Target Variable\n",
        "\n",
        "# Specifying the Target Variable\n",
        "target_variable_column = \"Purchase\"\n",
        "\n",
        "# Calculating the number of data points in each category and their percentage\n",
        "target_variable_counts = train_data.groupBy(target_variable_column).agg(\n",
        "    F.count(target_variable_column).alias(\"Count\"),\n",
        "    (F.count(target_variable_column) / train_data.count() * 100).alias(\"Percentage\")\n",
        ")\n",
        "\n",
        "# Displaying the results\n",
        "print(f\"Univariate Analysis for '{target_variable_column}':\")\n",
        "target_variable_counts.show()"
      ],
      "metadata": {
        "id": "MPxnVEopekWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba81945-78ed-432e-814c-d8c081cd454b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Univariate Analysis for 'Purchase':\n",
            "+--------+-----+--------------------+\n",
            "|Purchase|Count|          Percentage|\n",
            "+--------+-----+--------------------+\n",
            "|   11458|   57|0.010362355199720763|\n",
            "|    3794|   26|0.004726688336714733|\n",
            "|   15957|   28|0.005090279747231251|\n",
            "|    7880|  134|0.024360624504606705|\n",
            "|    2122|   32|0.005817462568264287|\n",
            "|    8086|  151|0.027451151493997104|\n",
            "|    9852|   89| 0.01617981776798505|\n",
            "|    7982|  160| 0.02908731284132144|\n",
            "|    5300|  131|0.023815237388831925|\n",
            "|   11858|   48| 0.00872619385239643|\n",
            "|    8592|   98| 0.01781597911530938|\n",
            "|    8638|  109| 0.01981573187315023|\n",
            "|   15447|   88| 0.01599802206272679|\n",
            "|    5156|  103|0.018724957641600676|\n",
            "|   15619|   77| 0.01399826930488594|\n",
            "|   10817|   17|0.003090526989390403|\n",
            "|   15846|   70|0.012725699368078129|\n",
            "|   12046|   52|0.009453376673429466|\n",
            "|    9900|  106|0.019270344757375452|\n",
            "|    3997|   35|0.006362849684039064|\n",
            "+--------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Average Purchase Amount\n",
        "\n",
        "# Calculating the average purchase amount\n",
        "average_purchase = train_data.agg(F.mean(target_variable_column).alias(\"Average Purchase\"))\n",
        "\n",
        "# Displaying the result\n",
        "average_purchase.show()"
      ],
      "metadata": {
        "id": "-mZ2WU4xeyeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83245c2-c127-4121-e108-093e2127d248"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "| Average Purchase|\n",
            "+-----------------+\n",
            "|9263.968712959126|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Missing Value Treatment"
      ],
      "metadata": {
        "id": "UPxfvE_9erfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NUll Values and Handling the same\n",
        "\n",
        "# For train data\n",
        "for column in train_data.columns:\n",
        "    # Defining the condition to check for null values in the column\n",
        "    missing_values = F.isnull(train_data[column])\n",
        "\n",
        "    # Filtering the data based on the condition and count the number of null values\n",
        "    missing_count = train_data.filter(missing_values).count()\n",
        "\n",
        "    # Printing the result\n",
        "    print(f\"Column '{column}': {missing_count} null values\")"
      ],
      "metadata": {
        "id": "o23xpzPPfEVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e4a332-8a44-412b-fdec-ed0052ed3a5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User_ID': 0 null values\n",
            "Column 'Product_ID': 0 null values\n",
            "Column 'Gender': 0 null values\n",
            "Column 'Age': 0 null values\n",
            "Column 'Occupation': 0 null values\n",
            "Column 'City_Category': 0 null values\n",
            "Column 'Stay_In_Current_City_Years': 0 null values\n",
            "Column 'Marital_Status': 0 null values\n",
            "Column 'Product_Category_1': 0 null values\n",
            "Column 'Product_Category_2': 173638 null values\n",
            "Column 'Product_Category_3': 383247 null values\n",
            "Column 'Purchase': 0 null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For test data (Removing the same in test data, so that for better consistency while predicting)\n",
        "for column in test_data.columns:\n",
        "    # Defining the condition to check for null values in the column\n",
        "    missing_values = F.isnull(test_data[column])\n",
        "\n",
        "    # Filtering the data based on the condition and count the number of null values\n",
        "    missing_count = test_data.filter(missing_values).count()\n",
        "\n",
        "    # Printing the result\n",
        "    print(f\"Column '{column}': {missing_count} null values\")"
      ],
      "metadata": {
        "id": "K1Lzwt3bfsUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a816dae-b4ec-4cbd-cb25-601a30472bac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User_ID': 0 null values\n",
            "Column 'Product_ID': 0 null values\n",
            "Column 'Gender': 0 null values\n",
            "Column 'Age': 0 null values\n",
            "Column 'Occupation': 0 null values\n",
            "Column 'City_Category': 0 null values\n",
            "Column 'Stay_In_Current_City_Years': 0 null values\n",
            "Column 'Marital_Status': 0 null values\n",
            "Column 'Product_Category_1': 0 null values\n",
            "Column 'Product_Category_2': 72344 null values\n",
            "Column 'Product_Category_3': 162562 null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Product_Category_2 and Product_Category_3 alone has null Values (Both In Train and Test Data)"
      ],
      "metadata": {
        "id": "v3oXUnyEf8Rt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Product Category represents the category to which they belong to.\n",
        "# So, it won't be appropriate to replace it mean and thus we are replacing it with 0 replacing null category"
      ],
      "metadata": {
        "id": "YayQ5oyFf_wQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the null values\n",
        "\n",
        "train_data = train_data.fillna({\n",
        "    \"Product_Category_2\": 0,  # Replacing null values in 'Product_Category_2' with 0\n",
        "    \"Product_Category_3\": 0   # Replacing null values in 'Product_Category_3' with 0\n",
        "})\n",
        "\n",
        "\n",
        "test_data = test_data.fillna({\n",
        "    \"Product_Category_2\": 0,  # Replacing null values in 'Product_Category_2' with 0\n",
        "    \"Product_Category_3\": 0   # Replacing null values in 'Product_Category_3' with 0\n",
        "})"
      ],
      "metadata": {
        "id": "AQWv5AtPgB7c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values after handling them\n",
        "\n",
        "# For train data after handling null values\n",
        "for column in train_data.columns:\n",
        "    missing_values = F.isnull(train_data[column])\n",
        "    missing_count = train_data.filter(missing_values).count()\n",
        "    print(f\"Train Data - Column '{column}': {missing_count} null values after handling\")\n",
        "\n",
        "# For test data after handling null values\n",
        "for column in test_data.columns:\n",
        "    missing_values = F.isnull(test_data[column])\n",
        "    missing_count = test_data.filter(missing_values).count()\n",
        "    print(f\"Test Data - Column '{column}': {missing_count} null values after handling\")"
      ],
      "metadata": {
        "id": "Gt3Wy417ghNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63a8086-f36c-481c-8e8b-76cbe4d33147"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data - Column 'User_ID': 0 null values after handling\n",
            "Train Data - Column 'Product_ID': 0 null values after handling\n",
            "Train Data - Column 'Gender': 0 null values after handling\n",
            "Train Data - Column 'Age': 0 null values after handling\n",
            "Train Data - Column 'Occupation': 0 null values after handling\n",
            "Train Data - Column 'City_Category': 0 null values after handling\n",
            "Train Data - Column 'Stay_In_Current_City_Years': 0 null values after handling\n",
            "Train Data - Column 'Marital_Status': 0 null values after handling\n",
            "Train Data - Column 'Product_Category_1': 0 null values after handling\n",
            "Train Data - Column 'Product_Category_2': 0 null values after handling\n",
            "Train Data - Column 'Product_Category_3': 0 null values after handling\n",
            "Train Data - Column 'Purchase': 0 null values after handling\n",
            "Test Data - Column 'User_ID': 0 null values after handling\n",
            "Test Data - Column 'Product_ID': 0 null values after handling\n",
            "Test Data - Column 'Gender': 0 null values after handling\n",
            "Test Data - Column 'Age': 0 null values after handling\n",
            "Test Data - Column 'Occupation': 0 null values after handling\n",
            "Test Data - Column 'City_Category': 0 null values after handling\n",
            "Test Data - Column 'Stay_In_Current_City_Years': 0 null values after handling\n",
            "Test Data - Column 'Marital_Status': 0 null values after handling\n",
            "Test Data - Column 'Product_Category_1': 0 null values after handling\n",
            "Test Data - Column 'Product_Category_2': 0 null values after handling\n",
            "Test Data - Column 'Product_Category_3': 0 null values after handling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for distinct values in each column\n",
        "\n",
        "# For train data\n",
        "distinct_counts_train = train_data.agg(*(F.countDistinct(F.col(column)).alias(column) for column in train_data.columns))\n",
        "\n",
        "# For test data\n",
        "distinct_counts_test = test_data.agg(*(F.countDistinct(F.col(column)).alias(column) for column in test_data.columns))\n",
        "\n",
        "# Displaying the output for train data\n",
        "print(\"Distinct Counts for Train Data:\")\n",
        "distinct_counts_train.show()\n",
        "\n",
        "# Displaying the output for test data\n",
        "print(\"Distinct Counts for Test Data:\")\n",
        "distinct_counts_test.show()"
      ],
      "metadata": {
        "id": "W1GrJ1JJgOyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885afff1-9581-4578-974c-d488b0417f6f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct Counts for Train Data:\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|   5891|      3631|     2|  7|        21|            3|                         5|             2|                20|                18|                16|   18105|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "\n",
            "Distinct Counts for Test Data:\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "|   5891|      3491|     2|  7|        21|            3|                         5|             2|                18|                18|                16|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting Category Values\n",
        "\n",
        "# Columns for analysis - Count Category Values\n",
        "count_category_columns_of_interest = [\"Gender\",\"Age\", \"City_Category\", \"Stay_In_Current_City_Years\", \"Marital_Status\"]\n",
        "\n",
        "# Function to compute distinct counts and group-wise counts\n",
        "def compute_counts(data, columns):\n",
        "    distinct_counts = data.agg(*(F.countDistinct(F.col(column)).alias(column) for column in columns))\n",
        "    distinct_counts.show()\n",
        "\n",
        "    for column in columns:\n",
        "        group_counts = data.groupBy(column).agg(F.count(column).alias(f\"{column}_count\"))\n",
        "        group_counts.show()\n",
        "\n",
        "# For train data\n",
        "print(\"Train Data:\")\n",
        "compute_counts(train_data, count_category_columns_of_interest)\n",
        "\n",
        "# For test data\n",
        "print(\"Test Data:\")\n",
        "compute_counts(test_data, count_category_columns_of_interest)"
      ],
      "metadata": {
        "id": "UT5O21y8hJu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f9426c-67ab-4e25-9c2a-4104a75292ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data:\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "|Gender|Age|City_Category|Stay_In_Current_City_Years|Marital_Status|\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "|     2|  7|            3|                         5|             2|\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "\n",
            "+------+------------+\n",
            "|Gender|Gender_count|\n",
            "+------+------------+\n",
            "|     F|      135809|\n",
            "|     M|      414259|\n",
            "+------+------------+\n",
            "\n",
            "+-----+---------+\n",
            "|  Age|Age_count|\n",
            "+-----+---------+\n",
            "|18-25|    99660|\n",
            "|26-35|   219587|\n",
            "| 0-17|    15102|\n",
            "|46-50|    45701|\n",
            "|51-55|    38501|\n",
            "|36-45|   110013|\n",
            "|  55+|    21504|\n",
            "+-----+---------+\n",
            "\n",
            "+-------------+-------------------+\n",
            "|City_Category|City_Category_count|\n",
            "+-------------+-------------------+\n",
            "|            B|             231173|\n",
            "|            C|             171175|\n",
            "|            A|             147720|\n",
            "+-------------+-------------------+\n",
            "\n",
            "+--------------------------+--------------------------------+\n",
            "|Stay_In_Current_City_Years|Stay_In_Current_City_Years_count|\n",
            "+--------------------------+--------------------------------+\n",
            "|                         3|                           95285|\n",
            "|                         0|                           74398|\n",
            "|                        4+|                           84726|\n",
            "|                         1|                          193821|\n",
            "|                         2|                          101838|\n",
            "+--------------------------+--------------------------------+\n",
            "\n",
            "+--------------+--------------------+\n",
            "|Marital_Status|Marital_Status_count|\n",
            "+--------------+--------------------+\n",
            "|             1|              225337|\n",
            "|             0|              324731|\n",
            "+--------------+--------------------+\n",
            "\n",
            "Test Data:\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "|Gender|Age|City_Category|Stay_In_Current_City_Years|Marital_Status|\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "|     2|  7|            3|                         5|             2|\n",
            "+------+---+-------------+--------------------------+--------------+\n",
            "\n",
            "+------+------------+\n",
            "|Gender|Gender_count|\n",
            "+------+------------+\n",
            "|     F|       57827|\n",
            "|     M|      175772|\n",
            "+------+------------+\n",
            "\n",
            "+-----+---------+\n",
            "|  Age|Age_count|\n",
            "+-----+---------+\n",
            "|18-25|    42293|\n",
            "|26-35|    93428|\n",
            "| 0-17|     6232|\n",
            "|46-50|    19577|\n",
            "|51-55|    16283|\n",
            "|36-45|    46711|\n",
            "|  55+|     9075|\n",
            "+-----+---------+\n",
            "\n",
            "+-------------+-------------------+\n",
            "|City_Category|City_Category_count|\n",
            "+-------------+-------------------+\n",
            "|            B|              98566|\n",
            "|            C|              72509|\n",
            "|            A|              62524|\n",
            "+-------------+-------------------+\n",
            "\n",
            "+--------------------------+--------------------------------+\n",
            "|Stay_In_Current_City_Years|Stay_In_Current_City_Years_count|\n",
            "+--------------------------+--------------------------------+\n",
            "|                         3|                           40143|\n",
            "|                         0|                           31318|\n",
            "|                        4+|                           35945|\n",
            "|                         1|                           82604|\n",
            "|                         2|                           43589|\n",
            "+--------------------------+--------------------------------+\n",
            "\n",
            "+--------------+--------------------+\n",
            "|Marital_Status|Marital_Status_count|\n",
            "+--------------+--------------------+\n",
            "|             1|               95792|\n",
            "|             0|              137807|\n",
            "+--------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Average Purchase\n",
        "\n",
        "average_purchase_columns_of_interest = [\"Gender\", \"Age\", \"City_Category\", \"Stay_In_Current_City_Years\", \"Marital_Status\"]\n",
        "\n",
        "for column in average_purchase_columns_of_interest:\n",
        "    avg_purchase = train_data.groupBy(column).agg(F.avg(\"Purchase\").alias(\"Average_Purchase\"))\n",
        "    print(f\"Average Purchase for '{column}':\")\n",
        "    avg_purchase.show()"
      ],
      "metadata": {
        "id": "Q6WtUAnShq4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe1efad-5b3c-494e-cfa6-0f07eb5c2935"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Purchase for 'Gender':\n",
            "+------+-----------------+\n",
            "|Gender| Average_Purchase|\n",
            "+------+-----------------+\n",
            "|     F|8734.565765155476|\n",
            "|     M|9437.526040472265|\n",
            "+------+-----------------+\n",
            "\n",
            "Average Purchase for 'Age':\n",
            "+-----+-----------------+\n",
            "|  Age| Average_Purchase|\n",
            "+-----+-----------------+\n",
            "|18-25|9169.663606261289|\n",
            "|26-35|9252.690632869888|\n",
            "| 0-17|8933.464640444974|\n",
            "|46-50|9208.625697468327|\n",
            "|51-55|9534.808030960236|\n",
            "|36-45|9331.350694917874|\n",
            "|  55+|9336.280459449405|\n",
            "+-----+-----------------+\n",
            "\n",
            "Average Purchase for 'City_Category':\n",
            "+-------------+-----------------+\n",
            "|City_Category| Average_Purchase|\n",
            "+-------------+-----------------+\n",
            "|            B|9151.300562781986|\n",
            "|            C| 9719.92099313568|\n",
            "|            A|8911.939216084484|\n",
            "+-------------+-----------------+\n",
            "\n",
            "Average Purchase for 'Stay_In_Current_City_Years':\n",
            "+--------------------------+-----------------+\n",
            "|Stay_In_Current_City_Years| Average_Purchase|\n",
            "+--------------------------+-----------------+\n",
            "|                         3|9286.904119221284|\n",
            "|                         0|9180.075122987177|\n",
            "|                        4+| 9275.59887165687|\n",
            "|                         1|9250.145923300364|\n",
            "|                         2|9320.429810090536|\n",
            "+--------------------------+-----------------+\n",
            "\n",
            "Average Purchase for 'Marital_Status':\n",
            "+--------------+-----------------+\n",
            "|Marital_Status| Average_Purchase|\n",
            "+--------------+-----------------+\n",
            "|             1|9261.174574082374|\n",
            "|             0|9265.907618921507|\n",
            "+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Label Encoding"
      ],
      "metadata": {
        "id": "dU3pudW4gLz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding the following columns: Age, Gender, Stay_In_Current_City_Years, City_Category\n",
        "\n",
        "# Doing encoding on both test and train data, so that we can evaluate the model performance in the test (unseen) data too.\n",
        "# Label Encoding - Assigning a unique integer to each of the categorical values Done using StringIndexer\n",
        "\n",
        "# Creating the StringIndexer Objects\n",
        "\n",
        "# Label encode \"Age\"\n",
        "age_indexer = StringIndexer(inputCol=\"Age\", outputCol=\"Age_le\", handleInvalid=\"skip\")\n",
        "\n",
        "# Label encode \"Gender\"\n",
        "gender_indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_le\", handleInvalid=\"skip\")\n",
        "\n",
        "# Label encode \"Stay_In_Current_City_Years\"\n",
        "stay_years_indexer = StringIndexer(inputCol=\"Stay_In_Current_City_Years\", outputCol=\"Stay_In_Current_City_Years_le\", handleInvalid=\"skip\")\n",
        "\n",
        "# Label encode \"City_Category\"\n",
        "city_category_indexer = StringIndexer(inputCol=\"City_Category\", outputCol=\"City_Category_le\", handleInvalid=\"skip\")"
      ],
      "metadata": {
        "id": "oO9py5S8kz0w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting StringIndexers on the train data\n",
        "age_indexer_model = age_indexer.fit(train_data)\n",
        "gender_indexer_model = gender_indexer.fit(train_data)\n",
        "stay_years_indexer_model = stay_years_indexer.fit(train_data)\n",
        "city_category_indexer_model = city_category_indexer.fit(train_data)\n",
        "\n",
        "# Transforming both train and test data using the fitted StringIndexers\n",
        "train_data_encoded = age_indexer_model.transform(train_data)\n",
        "train_data_encoded = gender_indexer_model.transform(train_data_encoded)\n",
        "train_data_encoded = stay_years_indexer_model.transform(train_data_encoded)\n",
        "train_data_encoded = city_category_indexer_model.transform(train_data_encoded)\n",
        "\n",
        "test_data_encoded = age_indexer_model.transform(test_data)\n",
        "test_data_encoded = gender_indexer_model.transform(test_data_encoded)\n",
        "test_data_encoded = stay_years_indexer_model.transform(test_data_encoded)\n",
        "test_data_encoded = city_category_indexer_model.transform(test_data_encoded)"
      ],
      "metadata": {
        "id": "JfI-blYhlqv4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. One-Hot Encoding"
      ],
      "metadata": {
        "id": "npbFYBQSgOtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot encoding following columns: Gender, City_Category, Occupation\n",
        "\n",
        "# One Hot Encoding - Converts categorical variables into binary columns, representing categories as binary values,\n",
        "#                    aiding machine learning models in understanding categorical data as numerical inputs\n",
        "\n",
        "# StringIndexer for 'Gender'\n",
        "gender_indexer = StringIndexer(inputCol='Gender', outputCol='Gender_Index')\n",
        "train_data_encoded = gender_indexer.fit(train_data_encoded).transform(train_data_encoded)\n",
        "test_data_encoded = gender_indexer.fit(test_data_encoded).transform(test_data_encoded)\n",
        "\n",
        "# StringIndexer for 'City_Category'\n",
        "city_category_indexer = StringIndexer(inputCol='City_Category', outputCol='CityCategoryIndex')\n",
        "train_data_encoded = city_category_indexer.fit(train_data_encoded).transform(train_data_encoded)\n",
        "test_data_encoded = city_category_indexer.fit(test_data_encoded).transform(test_data_encoded)\n",
        "\n",
        "# StringIndexer for 'Occupation'\n",
        "occupation_indexer = StringIndexer(inputCol='Occupation', outputCol='Occupation_Index')\n",
        "train_data_encoded = occupation_indexer.fit(train_data_encoded).transform(train_data_encoded)\n",
        "test_data_encoded = occupation_indexer.fit(test_data_encoded).transform(test_data_encoded)\n",
        "\n",
        "# OneHotEncoder for the indexed columns\n",
        "gender_encoder = OneHotEncoder(inputCols=['Gender_Index'], outputCols=['Gender_ohe'])\n",
        "city_category_encoder = OneHotEncoder(inputCols=['CityCategoryIndex'], outputCols=['City_Category_ohe'])\n",
        "occupation_encoder = OneHotEncoder(inputCols=['Occupation_Index'], outputCols=['Occupation_ohe'])\n",
        "\n",
        "train_data_encoded = gender_encoder.fit(train_data_encoded).transform(train_data_encoded)\n",
        "train_data_encoded = city_category_encoder.fit(train_data_encoded).transform(train_data_encoded)\n",
        "train_data_encoded = occupation_encoder.fit(train_data_encoded).transform(train_data_encoded)\n",
        "\n",
        "test_data_encoded = gender_encoder.fit(test_data_encoded).transform(test_data_encoded)\n",
        "test_data_encoded = city_category_encoder.fit(test_data_encoded).transform(test_data_encoded)\n",
        "test_data_encoded = occupation_encoder.fit(test_data_encoded).transform(test_data_encoded)"
      ],
      "metadata": {
        "id": "ys315Om2lyfx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the columns\n",
        "# We have encoded the following columns: \"Age\", \"Gender\", \"Stay_In_Current_City_Years\", \"City_Category\", \"Occupation\"\n",
        "# So, we can drop the original columns\n",
        "\n",
        "# Here, we have done both Label Encoding and One-Hot Encoding for 'Gender' and 'City_Category' columns\n",
        "# Using both might introduce multicollinearity, where the information in one column is redundant or correlated with another.\n",
        "# It will potentially affecting model performance\n",
        "# One-hot encoding creates binary categories, ideal for models needing categorical data in binary format\n",
        "# So, let us consider one-hot encoded columns for model building\n",
        "\n",
        "# For model building building, we won't be using User_ID and Product_ID and thus we can drop them\n",
        "\n",
        "columns_to_drop = [\"Age\", \"Gender\", \"Stay_In_Current_City_Years\", \"City_Category\", \"Occupation\",\n",
        "                   \"User_ID\",\"Product_ID\",\n",
        "                   \"Gender_le\", \"City_Category_le\"]\n",
        "train_data_encoded = train_data_encoded.drop(*columns_to_drop)\n",
        "test_data_encoded = test_data_encoded.drop(*columns_to_drop)"
      ],
      "metadata": {
        "id": "NP1hNJGyCIm6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Data Transformation"
      ],
      "metadata": {
        "id": "6ARVYY29gX2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the 'Purchase' column by taking logarithm to normalize the data\n",
        "train_data_encoded = train_data_encoded.withColumn(\"Purchase_log\", log1p(col(\"Purchase\")))\n",
        "\n",
        "# Dropping \"Purchase\" Column\n",
        "train_data_encoded = train_data_encoded.drop(\"Purchase\")"
      ],
      "metadata": {
        "id": "kpvfoGFJCOzT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_encoded.show()\n",
        "test_data_encoded.show()"
      ],
      "metadata": {
        "id": "mtCzrZA3CRJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4203586-e0b3-4eb5-c2eb-3708e3c97f31"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+-----------------+\n",
            "|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Age_le|Stay_In_Current_City_Years_le|Gender_Index|CityCategoryIndex|Occupation_Index|   Gender_ohe|City_Category_ohe| Occupation_ohe|     Purchase_log|\n",
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+-----------------+\n",
            "|             0|                 3|                 0|                 0|   6.0|                          1.0|         1.0|              2.0|            12.0|    (1,[],[])|        (2,[],[])|(20,[12],[1.0])|9.032528630660057|\n",
            "|             0|                 1|                 6|                14|   6.0|                          1.0|         1.0|              2.0|            12.0|    (1,[],[])|        (2,[],[])|(20,[12],[1.0])| 9.62911649414402|\n",
            "|             0|                12|                 0|                 0|   6.0|                          1.0|         1.0|              2.0|            12.0|    (1,[],[])|        (2,[],[])|(20,[12],[1.0])|7.260522598089852|\n",
            "|             0|                12|                14|                 0|   6.0|                          1.0|         1.0|              2.0|            12.0|    (1,[],[])|        (2,[],[])|(20,[12],[1.0])|6.964135612418245|\n",
            "|             0|                 8|                 0|                 0|   5.0|                          3.0|         0.0|              1.0|             9.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[9],[1.0])| 8.98343977178426|\n",
            "|             0|                 1|                 2|                 0|   0.0|                          2.0|         0.0|              2.0|            14.0|(1,[0],[1.0])|        (2,[],[])|(20,[14],[1.0])|9.630891117502388|\n",
            "|             1|                 1|                 8|                17|   3.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|9.863498544319764|\n",
            "|             1|                 1|                15|                 0|   3.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|9.671240186972447|\n",
            "|             1|                 1|                16|                 0|   3.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])| 9.66058762285624|\n",
            "|             1|                 8|                 0|                 0|   0.0|                          0.0|         0.0|              2.0|             5.0|(1,[0],[1.0])|        (2,[],[])| (20,[5],[1.0])|8.971067438732089|\n",
            "|             1|                 5|                11|                 0|   0.0|                          0.0|         0.0|              2.0|             5.0|(1,[0],[1.0])|        (2,[],[])| (20,[5],[1.0])|8.566935283311052|\n",
            "|             1|                 8|                 0|                 0|   0.0|                          0.0|         0.0|              2.0|             5.0|(1,[0],[1.0])|        (2,[],[])| (20,[5],[1.0])| 8.28349412616251|\n",
            "|             1|                 8|                 0|                 0|   0.0|                          0.0|         0.0|              2.0|             5.0|(1,[0],[1.0])|        (2,[],[])| (20,[5],[1.0])|8.711772645605695|\n",
            "|             1|                 1|                 2|                 5|   0.0|                          0.0|         0.0|              2.0|             5.0|(1,[0],[1.0])|        (2,[],[])| (20,[5],[1.0])|9.659248037927183|\n",
            "|             0|                 5|                 8|                14|   4.0|                          0.0|         1.0|              2.0|            19.0|    (1,[],[])|        (2,[],[])|(20,[19],[1.0])|8.590257762273243|\n",
            "|             0|                 4|                 5|                 0|   4.0|                          0.0|         1.0|              2.0|            19.0|    (1,[],[])|        (2,[],[])|(20,[19],[1.0])|7.640123172695364|\n",
            "|             0|                 2|                 3|                 4|   4.0|                          0.0|         1.0|              2.0|            19.0|    (1,[],[])|        (2,[],[])|(20,[19],[1.0])|9.477003077203888|\n",
            "|             0|                 5|                14|                 0|   4.0|                          0.0|         1.0|              2.0|            19.0|    (1,[],[])|        (2,[],[])|(20,[19],[1.0])| 9.08839870117094|\n",
            "|             1|                 1|                14|                16|   1.0|                          0.0|         0.0|              0.0|             3.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[3],[1.0])|9.374922172292127|\n",
            "|             1|                 1|                 5|                15|   0.0|                          3.0|         0.0|              1.0|             6.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[6],[1.0])|9.884049858643653|\n",
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+\n",
            "|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Age_le|Stay_In_Current_City_Years_le|Gender_Index|CityCategoryIndex|Occupation_Index|   Gender_ohe|City_Category_ohe| Occupation_ohe|\n",
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+\n",
            "|             1|                 1|                11|                 0|   3.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|\n",
            "|             0|                 3|                 5|                 0|   0.0|                          4.0|         0.0|              1.0|             4.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[4],[1.0])|\n",
            "|             1|                 5|                14|                 0|   1.0|                          3.0|         1.0|              0.0|             3.0|    (1,[],[])|    (2,[0],[1.0])| (20,[3],[1.0])|\n",
            "|             1|                 4|                 9|                 0|   1.0|                          3.0|         1.0|              0.0|             3.0|    (1,[],[])|    (2,[0],[1.0])| (20,[3],[1.0])|\n",
            "|             0|                 4|                 5|                12|   0.0|                          0.0|         1.0|              1.0|             3.0|    (1,[],[])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "|             1|                 2|                 3|                15|   3.0|                          2.0|         0.0|              1.0|             3.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "|             1|                 1|                11|                15|   3.0|                          2.0|         0.0|              1.0|             3.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "|             1|                 2|                 4|                 9|   3.0|                          2.0|         0.0|              1.0|             3.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "|             0|                10|                13|                16|   0.0|                          0.0|         0.0|              2.0|             2.0|(1,[0],[1.0])|        (2,[],[])| (20,[2],[1.0])|\n",
            "|             0|                 5|                14|                 0|   2.0|                          3.0|         0.0|              2.0|            14.0|(1,[0],[1.0])|        (2,[],[])|(20,[14],[1.0])|\n",
            "|             1|                 1|                 2|                15|   0.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|\n",
            "|             1|                 5|                 0|                 0|   0.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|\n",
            "|             1|                 5|                 8|                14|   0.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|\n",
            "|             1|                 5|                 8|                 0|   0.0|                          1.0|         0.0|              0.0|             2.0|(1,[0],[1.0])|    (2,[0],[1.0])| (20,[2],[1.0])|\n",
            "|             1|                10|                15|                16|   0.0|                          1.0|         1.0|              1.0|             3.0|    (1,[],[])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "|             0|                 2|                17|                 0|   1.0|                          0.0|         0.0|              1.0|             2.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[2],[1.0])|\n",
            "|             1|                15|                 0|                 0|   3.0|                          0.0|         0.0|              2.0|            11.0|(1,[0],[1.0])|        (2,[],[])|(20,[11],[1.0])|\n",
            "|             1|                 5|                 8|                14|   3.0|                          0.0|         0.0|              2.0|            11.0|(1,[0],[1.0])|        (2,[],[])|(20,[11],[1.0])|\n",
            "|             0|                 5|                 8|                 0|   2.0|                          4.0|         1.0|              2.0|             1.0|    (1,[],[])|        (2,[],[])| (20,[1],[1.0])|\n",
            "|             1|                 2|                 3|                 4|   3.0|                          3.0|         0.0|              1.0|             3.0|(1,[0],[1.0])|    (2,[1],[1.0])| (20,[3],[1.0])|\n",
            "+--------------+------------------+------------------+------------------+------+-----------------------------+------------+-----------------+----------------+-------------+-----------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10.Model Building"
      ],
      "metadata": {
        "id": "vs7wFSnUgceQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a baseline model using the ML algorithms\n",
        "\n",
        "# Let us build a \"Linear Regression\" model\n",
        "\n",
        "# Before passing the data into the ML model, we need to convert the required features into a Vector. We can do this using a VectorAssembler\n",
        "\n",
        "# Defining the input columns for assembling the feature vector\n",
        "input_cols = [\n",
        "    'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3',\n",
        "    'Age_le', 'Stay_In_Current_City_Years_le', 'Gender_ohe', 'City_Category_ohe', 'Occupation_ohe'\n",
        "]\n",
        "\n",
        "# Creating a VectorAssembler instance\n",
        "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
        "\n",
        "# Applying the VectorAssembler to our DataFrame (train_data_encoded)\n",
        "train_data_with_features = vector_assembler.transform(train_data_encoded)\n",
        "\n",
        "test_data_with_features = vector_assembler.transform(test_data_encoded)"
      ],
      "metadata": {
        "id": "o-m_33SmCX8_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Linear Regression model\n",
        "\n",
        "lr = LinearRegression(labelCol='Purchase_log')\n",
        "\n",
        "# Training the Linear Regression model\n",
        "lr_model = lr.fit(train_data_with_features)\n",
        "\n",
        "# Evaluating the model using RMSE on the training data\n",
        "evaluator = RegressionEvaluator(labelCol='Purchase_log', predictionCol='prediction', metricName='rmse')\n",
        "rmse_train = evaluator.evaluate(lr_model.transform(train_data_with_features))\n",
        "\n",
        "# Printing the RMSE on training data\n",
        "print(f\"Root Mean Squared Error (RMSE) on Training Data: {rmse_train}\")"
      ],
      "metadata": {
        "id": "TnY-qP-KCrlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418adfd8-94bf-4455-ad5f-e114a46c7cbe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on Training Data: 0.662419878439013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11. Making Predictions on the Test Data"
      ],
      "metadata": {
        "id": "KbEs_t2jgkh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_data_encoded' contains features similar to those in 'train_data_encoded'\n",
        "\n",
        "# Transforming the test data using the VectorAssembler\n",
        "test_data_with_features = vector_assembler.transform(test_data_encoded)\n",
        "\n",
        "# Making predictions on the test data using the trained model\n",
        "predictions_test = lr_model.transform(test_data_with_features)\n",
        "\n",
        "# Displaying predicted values from the test set\n",
        "predictions_test.select(\"prediction\").show()"
      ],
      "metadata": {
        "id": "JXNkgOrdCsz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869647dd-8ade-4747-8d97-e92ed8a23fcc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       prediction|\n",
            "+-----------------+\n",
            "|9.272006037282578|\n",
            "|9.133947458579556|\n",
            "|8.910557479788718|\n",
            "|8.978281797578164|\n",
            "|9.103891002771267|\n",
            "|9.331001861298244|\n",
            "|9.420422589371103|\n",
            "|9.280500219774536|\n",
            "|8.700131687880706|\n",
            "|8.993732925250736|\n",
            "| 9.37329074117509|\n",
            "|8.935249977596987|\n",
            "|9.070333005736552|\n",
            "|8.948601614694471|\n",
            "|8.700521069091872|\n",
            "|9.229617548905281|\n",
            "|8.179887895889117|\n",
            "|9.075661833782435|\n",
            "| 8.88416595902457|\n",
            "| 9.23557082247657|\n",
            "+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12.Grid Search CV - Model Improvement"
      ],
      "metadata": {
        "id": "oDPLEGOegrRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid-Search CV\n",
        "\n",
        "# Defining the parameter grid for tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Creating CrossValidator with the Linear Regression model, evaluation metric, and parameter grid\n",
        "cross_val = CrossValidator(estimator=lr,\n",
        "                           estimatorParamMaps=param_grid,\n",
        "                           evaluator=evaluator,\n",
        "                           numFolds=3)\n",
        "\n",
        "# Running cross-validation and select the best model\n",
        "cv_model = cross_val.fit(train_data_with_features)\n",
        "\n",
        "# Getting the best model from cross-validation\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Evaluating the best model on the training data\n",
        "predictions_train = best_model.transform(train_data_with_features)\n",
        "rmse_train_best = evaluator.evaluate(predictions_train)\n",
        "\n",
        "# Extracting the best model's parameters\n",
        "best_params = best_model.extractParamMap()\n",
        "\n",
        "# Getting the best model's parameters\n",
        "best_reg_param = best_model._java_obj.getRegParam()\n",
        "best_elastic_net_param = best_model._java_obj.getElasticNetParam()\n",
        "\n",
        "# Printing the best parameters\n",
        "print(f\"Best regParam: {best_reg_param}\")\n",
        "print(f\"Best elasticNetParam: {best_elastic_net_param}\")"
      ],
      "metadata": {
        "id": "bFO1iB8IC0_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29258e4-56f3-4a7e-b78b-8839c2e66650"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best regParam: 0.01\n",
            "Best elasticNetParam: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The regParam parameter represents the regularization strength, denoted as lambda (λ) in regularization formulas.\n",
        "# It's a non-negative value that determines the amount of regularization applied to the model.\n",
        "# A higher regParam value increases the regularization strength, helping prevent overfitting by penalizing large coefficient values in model\n",
        "\n",
        "# The elasticNetParam in PySpark's LinearRegression is a parameter that controls the mixture of L1 (Lasso) and L2 (Ridge) regularization in Elastic Net regularization.\n",
        "# An elasticNetParam of 0.0 means that the model has chosen Ridge Regression (L2 regularization) exclusively, disregarding Lasso (L1 regularization)."
      ],
      "metadata": {
        "id": "g3ccYYJFDWLL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: -\n",
        "\n",
        "# elasticNetParam is set to 1.0 --> L1 Regularization (Lasso Regression)\n",
        "# elasticNetParam is set to 0.0 --> L2 Regularization (Ridge Regression)"
      ],
      "metadata": {
        "id": "LrevPqR7DWo6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. Making Predictions on the Best Model"
      ],
      "metadata": {
        "id": "vfKJXWN0g3f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test data using the best model\n",
        "predictions_test_best = cv_model.bestModel.transform(test_data_with_features)\n",
        "\n",
        "# Displaying predicted values from the test set using the best model\n",
        "predictions_test_best.select(\"prediction\").show()"
      ],
      "metadata": {
        "id": "X75uPaO7DaNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c43233-af0e-4a6e-939b-0541ae3c933e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       prediction|\n",
            "+-----------------+\n",
            "|  9.2658342624479|\n",
            "|9.130565147619885|\n",
            "|8.909467006818046|\n",
            "|8.976011059171535|\n",
            "|9.103274702246212|\n",
            "| 9.32773762028767|\n",
            "|9.416147147493426|\n",
            "|9.276346646077075|\n",
            "|8.707816021472516|\n",
            "| 8.99240113987561|\n",
            "|9.369987686580545|\n",
            "|8.934126031341599|\n",
            "|9.071418553741198|\n",
            "|8.947581708173765|\n",
            "|   8.707357228611|\n",
            "|9.224764576254062|\n",
            "| 8.18953452144547|\n",
            "| 9.07636554758099|\n",
            "|8.883638513224229|\n",
            "|9.230642516969208|\n",
            "+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14. Spark ML Pipeline"
      ],
      "metadata": {
        "id": "W-NXarOGhu6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Spark ML Pipeline for the final model\n",
        "\n",
        "# Working Separately for the Pipeline for Generic Purpose\n",
        "\n",
        "# Reading the data\n",
        "\n",
        "train_data_pipeline = spark.read.csv(\"train.csv\", inferSchema=True, header=True)\n",
        "test_data_pipeline = spark.read.csv(\"test.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "aW4kZKkUFGXp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking and Replacing the null values in both train and test data\n",
        "# Getting the data types, to confirm if they are replaced accordingly\n",
        "\n",
        "# For train data\n",
        "train_data_pipeline.dtypes"
      ],
      "metadata": {
        "id": "iyVb8iRFILLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab7a755-d599-4b51-9ef9-d0b4a856c6ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('User_ID', 'int'),\n",
              " ('Product_ID', 'string'),\n",
              " ('Gender', 'string'),\n",
              " ('Age', 'string'),\n",
              " ('Occupation', 'int'),\n",
              " ('City_Category', 'string'),\n",
              " ('Stay_In_Current_City_Years', 'string'),\n",
              " ('Marital_Status', 'int'),\n",
              " ('Product_Category_1', 'int'),\n",
              " ('Product_Category_2', 'int'),\n",
              " ('Product_Category_3', 'int'),\n",
              " ('Purchase', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing null values for Train Data\n",
        "\n",
        "class MissingValueFiller_Train(Transformer, HasInputCol, HasOutputCol):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, outputCol=None, fill_values=None):\n",
        "        super(MissingValueFiller_Train, self).__init__()\n",
        "        self.fill_values = fill_values\n",
        "        self.inputCol = inputCol\n",
        "        self.outputCol = outputCol\n",
        "\n",
        "    def setFillValues(self, value):\n",
        "        self.fill_values = value\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        fill_values = self.fill_values\n",
        "\n",
        "        for col_name, fill_value in fill_values.items():\n",
        "            dataset = dataset.withColumn(col_name, when(col(col_name).isNull(), fill_value).otherwise(col(col_name)))\n",
        "\n",
        "        return dataset\n",
        "\n",
        "# Filling the values\n",
        "\n",
        "fill_values = {\n",
        "    \"User_ID\": 0,\n",
        "    \"Product_ID\": \"0\",\n",
        "    \"Gender\": \"M\",\n",
        "    \"Age\": \"0-17\",\n",
        "    \"Occupation\": 0,\n",
        "    \"City_Category\": \"A\",\n",
        "    \"Stay_In_Current_City_Years\": \"0\",\n",
        "    \"Marital_Status\": 0,\n",
        "    \"Product_Category_1\": 0,\n",
        "    \"Product_Category_2\": 0,\n",
        "    \"Product_Category_3\": 0,\n",
        "    \"Purchase\": 0                         # Target Column only for Train data\n",
        "}\n",
        "\n",
        "for col_name, fill_value in fill_values.items():\n",
        "    train_data_pipeline = train_data_pipeline.withColumn(col_name, when(col(col_name).isNull(), fill_value).otherwise(col(col_name)))"
      ],
      "metadata": {
        "id": "1PL2NUABFbe4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Null if handled accordingly for train data\n",
        "\n",
        "for c in train_data_pipeline.columns:\n",
        "    nan_count = train_data_pipeline.where(col(c).isNull()).count()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Column '{c}' has {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"Column '{c}' does not have any NaN values.\")"
      ],
      "metadata": {
        "id": "iqQYLqajIg5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41c5a1d-5a67-4c67-c468-64c5f4fa9c31"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User_ID' does not have any NaN values.\n",
            "Column 'Product_ID' does not have any NaN values.\n",
            "Column 'Gender' does not have any NaN values.\n",
            "Column 'Age' does not have any NaN values.\n",
            "Column 'Occupation' does not have any NaN values.\n",
            "Column 'City_Category' does not have any NaN values.\n",
            "Column 'Stay_In_Current_City_Years' does not have any NaN values.\n",
            "Column 'Marital_Status' does not have any NaN values.\n",
            "Column 'Product_Category_1' does not have any NaN values.\n",
            "Column 'Product_Category_2' does not have any NaN values.\n",
            "Column 'Product_Category_3' does not have any NaN values.\n",
            "Column 'Purchase' does not have any NaN values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing Null Values for Test Data\n",
        "\n",
        "class MissingValueFiller_Test(Transformer, HasInputCol, HasOutputCol):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, outputCol=None, fill_values=None):\n",
        "        super(MissingValueFiller_Test, self).__init__()\n",
        "        self.fill_values = fill_values\n",
        "        self.inputCol = inputCol\n",
        "        self.outputCol = outputCol\n",
        "\n",
        "    def setFillValues(self, value):\n",
        "        self.fill_values = value\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        fill_values = self.fill_values\n",
        "\n",
        "        for col_name, fill_value in fill_values.items():\n",
        "            dataset = dataset.withColumn(col_name, when(col(col_name).isNull(), fill_value).otherwise(col(col_name)))\n",
        "\n",
        "        return dataset\n",
        "\n",
        "# Filling the values\n",
        "\n",
        "fill_values = {\n",
        "    \"User_ID\": 0,\n",
        "    \"Product_ID\": \"0\",\n",
        "    \"Gender\": \"M\",\n",
        "    \"Age\": \"0-17\",\n",
        "    \"Occupation\": 0,\n",
        "    \"City_Category\": \"A\",\n",
        "    \"Stay_In_Current_City_Years\": \"0\",\n",
        "    \"Marital_Status\": 0,\n",
        "    \"Product_Category_1\": 0,\n",
        "    \"Product_Category_2\": 0,\n",
        "    \"Product_Category_3\": 0\n",
        "}\n",
        "\n",
        "for col_name, fill_value in fill_values.items():\n",
        "    test_data_pipeline = test_data_pipeline.withColumn(col_name, when(col(col_name).isNull(), fill_value).otherwise(col(col_name)))"
      ],
      "metadata": {
        "id": "ECcgv4fZIm8O"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Null if handled accordingly for test data\n",
        "\n",
        "for c in test_data_pipeline.columns:\n",
        "    nan_count = test_data_pipeline.where(col(c).isNull()).count()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Column '{c}' has {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"Column '{c}' does not have any NaN values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1XvFGlugGeg",
        "outputId": "080ab85f-6cb1-48de-ad95-dbe14fd42130"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User_ID' does not have any NaN values.\n",
            "Column 'Product_ID' does not have any NaN values.\n",
            "Column 'Gender' does not have any NaN values.\n",
            "Column 'Age' does not have any NaN values.\n",
            "Column 'Occupation' does not have any NaN values.\n",
            "Column 'City_Category' does not have any NaN values.\n",
            "Column 'Stay_In_Current_City_Years' does not have any NaN values.\n",
            "Column 'Marital_Status' does not have any NaN values.\n",
            "Column 'Product_Category_1' does not have any NaN values.\n",
            "Column 'Product_Category_2' does not have any NaN values.\n",
            "Column 'Product_Category_3' does not have any NaN values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking Log Transformation for the target column\n",
        "\n",
        "train_data_pipeline = train_data_pipeline.withColumn(\"Purchase_log\", log1p(col(\"Purchase\")))"
      ],
      "metadata": {
        "id": "QIgvFhq4gMFh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Stages for our Pipeline\n",
        "# Defining the stages\n",
        "\n",
        "# Stage 1: Label Encoding for 'Age' Column\n",
        "stage_1 = StringIndexer(inputCol=\"Age\", outputCol=\"Age_Encoded\")\n",
        "\n",
        "# Stage 2: Label Endoing for 'Stay in Current City Years' Column\n",
        "stage_2 = StringIndexer(inputCol=\"Stay_In_Current_City_Years\", outputCol=\"Stay_In_Current_City_Years_Encoded\")\n",
        "\n",
        "# Stage 3: Label Encoding for 'Gender' Column\n",
        "stage_3 = StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_Encoded\")\n",
        "\n",
        "# Stage 4: Label Encoding for 'City_Category' Column\n",
        "stage_4 = StringIndexer(inputCol=\"City_Category\", outputCol=\"City_Category_Encoded\")\n",
        "\n",
        "# Stage 5: One-Hot Encoding for 'Occupation' Column\n",
        "stage_5 = OneHotEncoder(inputCol=\"Occupation\", outputCol=\"Occupation_Encoded\")\n",
        "\n",
        "# Stage 6: Creating a VectorAssembler for features\n",
        "stage_6 = VectorAssembler(inputCols=[\n",
        "    'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3',\n",
        "    'Age_Encoded', 'Stay_In_Current_City_Years_Encoded', 'Gender_Encoded', 'City_Category_Encoded',\n",
        "    'Occupation_Encoded'],\n",
        "    outputCol=\"feature_vector\")\n",
        "\n",
        "# Stage 7: Model Building\n",
        "\n",
        "# We have found out our best model, is with regParam=0.01 and elasticNetParam=0.0\n",
        "# Thus, we are building our model with best parameters\n",
        "\n",
        "model = LinearRegression(featuresCol=\"feature_vector\", labelCol=\"Purchase_log\")\n",
        "\n",
        "# Setting regParam and elasticNetParam using setParams method\n",
        "best_linear_regression_model = model.setParams(regParam=0.01, elasticNetParam=0.0)\n",
        "\n",
        "stage_7 = best_linear_regression_model"
      ],
      "metadata": {
        "id": "vYynGstwgS7e"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the stages\n",
        "stages = [\n",
        "    stage_1,\n",
        "    stage_2,\n",
        "    stage_3,\n",
        "    stage_4,\n",
        "    stage_5,\n",
        "    stage_6,\n",
        "    stage_7\n",
        "]"
      ],
      "metadata": {
        "id": "wAeZCBeWgWxR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the pipeline and fitting the model\n",
        "\n",
        "# Creating a pipeline with defined stages\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# Fitting the pipeline on our training data\n",
        "pipeline_model = pipeline.fit(train_data_pipeline)"
      ],
      "metadata": {
        "id": "LS-sttjyg-hh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the test data\n",
        "\n",
        "# Transformation\n",
        "predictions=pipeline_model.transform(test_data_pipeline)"
      ],
      "metadata": {
        "id": "2UPOSxIqgbeD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting output of predicted values\n",
        "\n",
        "# Action\n",
        "predictions.select(\"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gcAlFP2gzZ7",
        "outputId": "8dcd99ab-7f3d-4816-de12-d415fa0db949"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       prediction|\n",
            "+-----------------+\n",
            "|9.280117070287089|\n",
            "|9.102684532282222|\n",
            "|8.918940260556024|\n",
            "|8.985328349102224|\n",
            "| 9.07367654328683|\n",
            "|9.303449219731071|\n",
            "|9.392134846784627|\n",
            "|9.251532934917675|\n",
            "|8.723775759056595|\n",
            "|9.009692713879229|\n",
            "| 9.38028531905237|\n",
            "|8.942919968724068|\n",
            "|9.081781651076557|\n",
            "|8.956641530882441|\n",
            "| 8.67892462730022|\n",
            "| 9.19724592872509|\n",
            "|8.207340620481414|\n",
            "|9.095842951785723|\n",
            "| 8.89967782282214|\n",
            "| 9.20513165688947|\n",
            "+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In this Project, we have built a Machine Learning Model using Spark ML for Predicting the Purchase Amount of customers for a Retail Company."
      ],
      "metadata": {
        "id": "3EXwjSE5h_2y"
      }
    }
  ]
}